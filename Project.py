# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QseuySwNzE7pfRI280rQsuu9XGpAgz7J
"""

import ipywidgets as widgets # iport the library of GUI
from IPython.display import display # For dispalying the GUI
import spacy # A library used for reconizing the name and entities
from textblob import TextBlob # A library used for Sentment analysis
from transformers import pipeline, MarianMTModel, MarianTokenizer # A library contianing clases that support pipeline class for text summrization and Maraine M tokenizer fo translation.

# Load NLP models
nlp = spacy.load("en_core_web_sm")  # For entity name recognition NER
summarizer = pipeline("summarization")  # For summarization, were get form transformer library
translator = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-ar") # Takes the english and transform it to tokens
translator_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-ar")# takes the English tokens and transform it to Arabic

# GUI Components
text_input = widgets.Textarea(description="Document:", layout={'width': '600px', 'height': '200px'})
process_button = widgets.Button(description="Process Document")
output_area = widgets.Output()

def process_document(b):
    # Clear previous output
    output_area.clear_output()

    with output_area:# display any print function in the output area
        document = text_input.value #take the vlaue put in the documnt varibel

        # Named Entity Recognition
        doc = nlp(document)# Name/entiy recognition output in doc varible
        entities = [(ent.text, ent.label_) for ent in doc.ents]# for each entity in the doc, dispaly the entities in terms of text and labels
        print("Entities:", entities) # print entites

        # Sentiment Analysis
        sentiment = TextBlob(document).sentiment # extract the sentiment using the Textblob and put in sentimet variables.
        print("Sentiment: Positive" if sentiment.polarity > 0 else "Sentiment: Negative" if sentiment.polarity < 0 else "Sentiment: Neutral") # depding on polarity, print the sentiment analysis.

        # Text Summarization
        summary = summarizer(document, max_length=130, min_length=30, do_sample=False)[0]['summary_text']
        print("Summary:", summary) # afunction take 4 inputs : document, max lenh, min lenth, do sample (false) to return a couple of words relevent to the text in random, of [0] returen the first summary, get the content of the summary text

        # Translation to Arabic
        translation_inputs = translator_tokenizer(document, return_tensors="pt", padding=True, truncation=True, max_length=512)# breaks the document into tokens, sotores the tokens in adata structurs called tensors
        translations = translator.generate(**translation_inputs)
        translation = translator_tokenizer.decode(translations[0], skip_special_tokens=True)
        print("Translation to Arabic:", translation)

# Bind the button click event
process_button.on_click(process_document)

# Display GUI
display(text_input, process_button, output_area)

pip install tensorflow

!pip install datasets
!pip install keras-tcn
import ipywidgets as widgets
from IPython.display import display, clear_output
import numpy as np # import nmpy library
from tensorflow.keras.datasets import imdb #
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, GlobalMaxPooling1D, SpatialDropout1D, GRU, TimeDistributed, Bidirectional, SimpleRNN,Input
from tcn import TCN
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
from datasets import load_dataset
import tensorflow as tf
from transformers import AutoTokenizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from datasets import load_dataset

# Initialize global variables to store data and models
X_pad, y, max_len, vocab_size, num_tags, tokenizer, tag_tokenizer = [None]*7
gru_model, bilstm_model = None, None

# Define function to load and preprocess IMDb dataset
def load_imdb_dataset():
    global X_pad, y
    top_words = 10000
    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)
    X = np.concatenate((X_train, X_test), axis=0)
    y = np.concatenate((y_train, y_test), axis=0)
    max_review_length = 500
    X_pad = pad_sequences(X, maxlen=max_review_length)

# Define function to load and preprocess the NER dataset
def load_ner_dataset():
    global X_train, y_train, max_len, vocab_size, num_tags, tokenizer, tag_tokenizer

    # Load the dataset
    dataset = load_dataset("conll2003", split='train')

    # Prepare text data (tokens) and labels (NER tags)
    X_train = [entry['tokens'] for entry in dataset]
    y_train = [entry['ner_tags'] for entry in dataset]

    # Tokenizer for the textual data
    tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
    tokenizer.fit_on_texts(X_train)
    X_train = tokenizer.texts_to_sequences(X_train)
    max_len = max(len(x) for x in X_train)  # Maximum length for padding
    X_train = pad_sequences(X_train, maxlen=max_len, padding='post')

    # Handling tags without using text tokenizer
    tag_tokenizer = {tag: idx for idx, tag in enumerate(set(tag for sublist in y_train for tag in sublist))}
    y_train = [[tag_tokenizer[tag] for tag in seq] for seq in y_train]
    y_train = pad_sequences(y_train, maxlen=max_len, padding='post')  # Ensure all output sequences are the same length
    y_train = to_categorical(y_train, num_classes=len(tag_tokenizer))  # One-hot encode the tags

    vocab_size = len(tokenizer.word_index) + 1  # Update vocabulary size
    num_tags = len(tag_tokenizer)  # Update number of tags

    return X_train  # Typically, you might not need to return this if you're setting globals

# Widget for task selection
task_selector = widgets.Dropdown(
    options=['Select Task', 'Sentiment Analysis', 'Name Entity Recognition','Text Summarization','Machine translation'],
    description='Task:',
)

# Widget for dataset selection, initially empty
dataset_selector = widgets.Dropdown(
    options=[],
    description='Dataset:',
)

# Widget for model selection, initially empty
model_selector = widgets.Dropdown(
    options=[],
    description='Model:',
)

# Button to execute the task
execute_button = widgets.Button(description='Execute Task', button_style='success')

# Output area
output_area = widgets.Output()

# Update dataset options based on task selection
def update_dataset_options(change):
    if task_selector.value == 'Sentiment Analysis':
        dataset_selector.options = [('IMDb Reviews', 'imdb')]
    elif task_selector.value == 'Name Entity Recognition':
        dataset_selector.options = [('CONLL2003', 'conll2003')]
    elif task_selector.value == 'Text Summarization':
        dataset_selector.options = [('cnn_dailymail', 'cnn_dailymail')]
    elif task_selector.value == 'Machine translation':
        dataset_selector.options = [('Helsinki-NLP/opus-100', 'Helsinki-NLP/opus-100')]
    else:
        dataset_selector.options = []

task_selector.observe(update_dataset_options, names='value')

# Update model options based on dataset selection
def update_model_options(change):
    if dataset_selector.value == 'imdb':
        model_selector.options = [('LSTM', 'LSTM'), ('1D CNN', '1D CNN')]
    elif dataset_selector.value == 'conll2003':
        model_selector.options = [('GRU', 'GRU'), ('BiLSTM', 'BiLSTM')]
    elif dataset_selector.value == 'cnn_dailymail':
        model_selector.options = [('RNN', 'RNN'), ('Bi-RNN', 'Bi-RNN')]
    elif dataset_selector.value == 'Helsinki-NLP/opus-100':
        model_selector.options = [('BiGRU', 'BiGRU'), ('TCN', 'TCN')]
    else:
        model_selector.options = []

dataset_selector.observe(update_model_options, names='value')

# Function to train and evaluate models for Sentiment Analysis
def train_evaluate_sentiment_analysis(model_type, X, y):
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)
    top_words = 10000;
    max_review_length = 500
    # Define the model
    model = Sequential()
    model.add(Embedding(input_dim=top_words, output_dim=128, input_length=max_review_length))

    if model_type == 'LSTM':
        model.add(LSTM(100))
    elif model_type == '1D CNN':
        model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
        model.add(GlobalMaxPooling1D())
        model.add(Dense(250, activation='relu'))

    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Train the model (Placeholder: Replace with your training code)
    print(f"Training {model_type} model on IMDb dataset...")
    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=64)

    # Evaluate the model (Placeholder: Replace with your evaluation code)
    accuracy = np.random.rand()  # Placeholder for actual model evaluation
    print(f"{model_type} Model Accuracy on IMDb dataset: {accuracy*100:.2f}%")
    return model

# Function to train and evaluate models for NER
def train_evaluate_ner(model_type, X_train, y_train, max_len, vocab_size, num_tags):
    # Define the model
    if model_type == 'GRU':
        model = Sequential([
            Embedding(input_dim=vocab_size, output_dim=50, input_length=max_len),
            SpatialDropout1D(0.2),
            GRU(128, return_sequences=True),
            TimeDistributed(Dense(num_tags, activation='softmax'))
        ])
    elif model_type == 'BiLSTM':
        model = Sequential([
            Embedding(input_dim=vocab_size, output_dim=50, input_length=max_len),
            SpatialDropout1D(0.2),
            Bidirectional(LSTM(128, return_sequences=True)),
            TimeDistributed(Dense(num_tags, activation='softmax'))
        ])

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


    # Train and evaluate the model (Placeholder: Replace with your training and evaluation code)
    print(f"Training {model_type} model on CONLL2003 dataset...")
    model.fit(X_train, y_train, epochs=1, batch_size=64)
    accuracy = np.random.rand()  # Placeholder for actual model evaluation
    print(f"{model_type} Model Accuracy on CONLL2003 dataset: {accuracy*100:.2f}%")
    return model

def create_text_summarization_model(model_type, input_length):
    model = Sequential()
    model.add(Embedding(input_dim=10000, output_dim=64, input_length=input_length))

    if model_type == 'RNN':
        model.add(SimpleRNN(128))
    elif model_type == 'Bi-RNN':
        model.add(Bidirectional(SimpleRNN(128)))
    else:
        raise ValueError("Unsupported model type")

    model.add(Dense(64, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))  # Placeholder for demonstration
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model

def load_text_summarization_data():
    dataset = load_dataset("cnn_dailymail", "3.0.0", split='train[:1%]')
    articles = [item['article'] for item in dataset]
    # Summaries are not used in this simplified example

    tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
    tokenizer.fit_on_texts(articles)
    sequences = tokenizer.texts_to_sequences(articles)
    padded = pad_sequences(sequences, maxlen=500, padding='post', truncating='post')

    # Splitting dataset
    train_padded, test_padded = train_test_split(padded, test_size=0.4, random_state=42)
    return train_padded, test_padded

def load_translate_data():
    # Load dataset
    dataset = load_dataset('Helsinki-NLP/opus-100', 'en-mr')
    en_sentences = [pair['en'] for pair in dataset['train']['translation']]
    mr_sentences = [pair['mr'] for pair in dataset['train']['translation']]

    # Tokenize sentences
    tokenizer_en = Tokenizer(num_words=10000, oov_token="<OOV>")
    tokenizer_en.fit_on_texts(en_sentences)
    sequences_en = tokenizer_en.texts_to_sequences(en_sentences)
    padded_en = pad_sequences(sequences_en, padding='post')

    tokenizer_mr = Tokenizer(num_words=10000, oov_token="<OOV>")
    tokenizer_mr.fit_on_texts(mr_sentences)
    sequences_mr = tokenizer_mr.texts_to_sequences(mr_sentences)
    padded_mr = pad_sequences(sequences_mr, padding='post', maxlen=padded_en.shape[1])


    LX_train, X_test, y_train, y_test = train_test_split(padded_en, np.expand_dims(padded_mr, -1), test_size=0.4, random_state=42)

    mr_vocab_size = len(tokenizer_mr.word_index) + 1  # Marathi vocabulary size

    input_dim = 10000  # English vocabulary size
    embedding_dim = 256
    max_length = padded_en.shape[1]
    return input_dim,embedding_dim,max_length,mr_vocab_size,LX_train,X_test,y_train,y_test

def load_translate_models (modeltype, input_dim, embedding_dim, max_length,mr_vocab_size):
     if modeltype == 'BiGRU':
        model = Sequential([
                Embedding(input_dim=input_dim, output_dim=embedding_dim, input_length=max_length),
                Bidirectional(GRU(128, return_sequences=True)),
                Dense(mr_vocab_size, activation='softmax')
                ])
     elif modeltype == 'TCN':
           input_layer = Input(shape=(max_length,))
           embedding_layer = Embedding(input_dim=input_dim, output_dim=embedding_dim, input_length=max_length)(input_layer)
           tcn_layer = TCN(return_sequences=True)(embedding_layer)
           output_layer = Dense(mr_vocab_size, activation='softmax')(tcn_layer)
           model = Model(inputs=[input_layer], outputs=[output_layer])
     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
     return model

def preprocess_text(data):
    vocab_size = 10000
    max_length = 500
    trunc_type = 'post'
    padding_type = 'post'
    oov_tok = "<OOV>"

    # Tokenize dataset
    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
    tokenizer.fit_on_texts(data)
    sequences = tokenizer.texts_to_sequences(data)
    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
    return padded

def preprocess_text_ner(text, tokenizer, max_length):
    sequence = tokenizer.texts_to_sequences([text])
    padded_sequence = pad_sequences(sequence, maxlen=max_length)  # Adjust to the same max_length as during training
    return padded_sequence



def decode_ner_predictions(predictions):
    tags = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']  # Example tags
    tag_to_index = {tag: idx for idx, tag in enumerate(tags)}
    index_to_tag = {idx: tag for tag, idx in tag_to_index.items()}

    # Ensure predictions are numpy array in case they aren't
    if not isinstance(predictions, np.ndarray):
        predictions = np.array(predictions)

    # predictions.argmax(axis=-1) should be 2D if predictions are from a batch
    pred_indices = predictions.argmax(axis=-1)  # This should be 2D

    # Decode each set of predictions in the batch
    pred_tags = [[index_to_tag[idx] for idx in sequence] for sequence in pred_indices]
    return pred_tags


text_input = widgets.Textarea(
    value='',
    placeholder='Type something',
    description='Input Text:',
    disabled=False,
    layout={'width': '100%'}
)

# Define function to execute the selected task and model
def execute_task(b):
    input_text = text_input.value
    global X_train, y_train, max_len, vocab_size, num_tags
    with output_area:
        clear_output()
        if input_text.strip() == "":
          print("Please enter some text to analyze")
          return
        if task_selector.value == 'Sentiment Analysis' and dataset_selector.value == 'imdb':
            load_imdb_dataset()
            print(f"Loaded IMDb dataset with {X_pad.shape[0]} samples.")
            sentiment_model = train_evaluate_sentiment_analysis(model_selector.value, X_pad, y)
            processed_text = preprocess_text(input_text)  # Assume 'tokenizer' and '500' are defined globally
            sentiment = sentiment_model.predict(processed_text)[0]
            print(sentiment)
            print(f"Sentiment: {'Positive' if sentiment > 0.5 else 'Negative'}")
        elif task_selector.value == 'Name Entity Recognition' and dataset_selector.value == 'conll2003':
            load_ner_dataset()
            print(f"Loaded CONLL2003 dataset with {len(X_train)} samples.")
            ner_model = train_evaluate_ner(model_selector.value, X_train, y_train, max_len, vocab_size, num_tags)
            processed_text = preprocess_text_ner(input_text,tokenizer,max_len)  # Ensure this function is defined
            predictions = ner_model.predict(processed_text)
            decoded_tags = decode_ner_predictions(predictions)
            for sequence in decoded_tags:
                print("Decoded sequence:", ', '.join(sequence))
        elif task_selector.value == 'Text Summarization' and dataset_selector.value == 'cnn_dailymail':
            train_padded, test_padded = load_text_summarization_data()  # Define this function to load your data
            summarization_model = create_text_summarization_model(model_selector.value, 500)  # Example input_length=500
            # Placeholder target for demonstration; replace with your actual targets
            target = np.zeros(len(train_padded))
            summarization_model.fit(train_padded, target, epochs=1, batch_size=64, verbose=1)
            print(f"Trained {model_selector.value} model on Text Summarization dataset.")
            processed_text = preprocess_text(input_text)  # Assume 'tokenizer' and '500' are suitable
            summary = summarization_model.predict(processed_text)
            print("Summary:", summary)
        elif task_selector.value == 'Machine translation' and dataset_selector.value == 'Helsinki-NLP/opus-100':
            print(f"Trained {model_selector.value} model on Text translation dataset.")
            input_dim,embedding_dim,max_length,mr_vocab_size,X_train,X_test,y_train,y_test = load_translate_data()  # Define this function to load your data
            translation_model = load_translate_models (model_selector.value, input_dim, embedding_dim, max_length,mr_vocab_size)
            translation_model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test))
            processed_text = preprocess_text(input_text)  # Assume tokenizer and max_length are defined for translation
            translation = translation_model.predict(processed_text)
            print("Translation:", translation)
execute_button.on_click(execute_task)

# Display widgets
display(text_input,task_selector, dataset_selector, model_selector, execute_button, output_area)